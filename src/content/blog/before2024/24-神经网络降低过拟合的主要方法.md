---
author: 公子小白
pubDatetime: 2020-07-09T23:18:00Z
title: 神经网络降低过拟合的主要方法
postSlug: "24"
featured: false
draft: false
tags:
  - "keras"
  - "deep-learning"
description: 神经网络降低过拟合的主要方法
---

神经网络往往会因为各种原因表现出过拟合的现象，如数据量太少、模型参数设置不合理等。解决神经网络的过拟合问题通常有以下几种方法，在一定程度上可以降低过拟合：

1. 获取更多训练数据
2. 减小网络大小
3. 添加权重正则化
4. 添加dropout

**1.获取更多训练数据**

获取更多训练数据是在条件允许的情况下**最优**的解决方法。模型训练数据越多，泛化能力就越强。

**2.减小模型大小**

减小网络大小是降低过拟合**最简单**的技巧，即减少模型中可学习参数的个数。

直观上来看，参数更多的模型拥有更大的记忆容量，因此在训练集上的表现会很好，但在验证集上的泛化能力不强。这样的模型毫无实际用处。

而如果模型太小，则记忆能力不足，有可能会导致欠拟合。

需要在二者之间找到一个平衡。

**3.添加权重正则化**

权重正则化是一种**较常见**的降低过拟合方法。通过强制让模型权重只能取较小的值，从而降低模型的复杂度，这使得权重值的发布更加规则。有两种形式：

1. **L1正则化：**添加的成本与权重系数的绝对值成正比。
2. **L2正则化：**添加的成本与权重系数的平方成正比。神经网络的L2正则化也叫做权重衰减。

权重正则化在Keras里的实现：

```python
model.add(layers.LSTM(64, activation='relu',
                     kernel_regularizer=regularizers.l2(0.001)))
#L2权重正则化，表示该层权重矩阵的每个系数都会使网络总损失增加0.001*weight_cofficient_value。
```

**4.使用dropout正则化**

dropout是神经网络最有效也最常用的正则化方法之一。对某一层使用dropout，就是在训练过程中随机对该层的一些输出特征舍弃。dropout rate是被设为0的特征所占的比例，一般在0.2~0.5之间。

dropout在Keras里的具体实现：

```python
#直接添加Dropout层
model.add(layers.Dropout(0.2))
#在Dense层中添加
model.add(layers.Dense(64,dropout=0.2))
#在循环层中添加
model.add(layers.LSTM(64,
                     dropout=0.2,
                     recurrent_dropout=0.5))
```

\***提示：**

在对数据进行标准化时，只能使用训练集的均值和标准差。否则会泄露验证集的“信息”，导致结果过拟合。

**参考文献**：

- 《python深度学习》——\[美\] 弗朗索瓦·肖莱
